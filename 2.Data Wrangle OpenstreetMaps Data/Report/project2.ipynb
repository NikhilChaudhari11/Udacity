{
 "metadata": {
  "name": "",
  "signature": "sha256:cf878fe4f59cfc4dfb56f79a1754cd17e47c1efed375fed514d1ea91da31405b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data Wrangling with MongoDB - Final Project"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Yachen Yan"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1. Map Area Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Map Area**: State College, Pennsylvania  \n",
      "**Data Download Link**: https://s3.amazonaws.com/metro-extracts.mapzen.com/state-college_pennsylvania.osm.bz2  \n",
      "**Introduction**: The community is a college town, dominated economically and demographically by the presence of the University Park campus of the Pennsylvania State University (from Wikipedia). I choose this area because I personally have been there once to join NE Chinese Soccer Tournament. It's a  quaint, rustic place.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Problems Encountered in the Map"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After donwloading the data and run and audit.py for initially data checking, I have found 3 main problems in the data.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load audit.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import xml.etree.cElementTree as ET\n",
      "from collections import defaultdict\n",
      "import re\n",
      "import pprint\n",
      "\n",
      "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
      "\n",
      "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \"Trail\", \"Parkway\", \"Commons\", \n",
      "            \"Highway\", \"Park\", \"Way\", \"Freeway\", \"Circle\", \"Cove\", \"Alley\", \"Pike\", \"East\", \"West\", \"North\", \"South\"]\n",
      "\n",
      "# UPDATE THIS VARIABLE\n",
      "mapping = { \"St\": \"Street\",\n",
      "            \"St.\": \"Street\",\n",
      "            \"ln\": \"Lane\",\n",
      "            \"ln.\": \"Lane\",\n",
      "            \"Ln\": \"Lane\",\n",
      "            \"Ln.\": \"Lane\",\n",
      "            \"Ave\": \"Avenue\",\n",
      "            \"Avene\":\"Avenue\",\n",
      "            \"Rd.\":\"Road\",\n",
      "            \"Rd\":\"Road\",\n",
      "            \"RD\":\"Road\",\n",
      "            \"Dr\": \"Drive\",\n",
      "            \"Dr.\": \"Drive\",\n",
      "            \"Cir\":\"Circle\",\n",
      "            \"Blvd\":\"Boulevard\",\n",
      "            \"Blvd.\":\"Boulevard\",\n",
      "            \"Blvd,\":\"Boulevard\",\n",
      "            \"Blvd.,\":\"Boulevard\",\n",
      "            \"Cv\":\"Cove\",\n",
      "            \"Hwy\": \"Highway\",\n",
      "            \"Hwy.\": \"Highway\",\n",
      "            \"Ct\":\"Court\",\n",
      "            \"Ctr\":\"Court\",\n",
      "            \"CR\":\"Court\",\n",
      "            \"Ln\":\"Lane\",\n",
      "            \"Pkwy\":\"Parkway\",\n",
      "            \"Pky\": \"Parkway\",\n",
      "            \"Pky.\": \"Parkway\",\n",
      "            \"Fwy\": \"Freeway\",\n",
      "            \"Fwy.\": \"Freeway\",\n",
      "            \"E\": \"East\",\n",
      "            \"W\": \"West\",\n",
      "            \"N\": \"North\",\n",
      "            #\"S\": \"South\",\n",
      "            }\n",
      "\n",
      "def audit_street_type(street_types, street_name):\n",
      "    m = street_type_re.search(street_name)\n",
      "    if m:\n",
      "        street_type = m.group()\n",
      "        if street_type not in expected:\n",
      "            street_types[street_type].add(street_name)\n",
      "\n",
      "def is_street_name(elem):\n",
      "    return (elem.attrib['k'] == \"addr:street\")\n",
      "\n",
      "def audit(osmfile):\n",
      "    osm_file = open(osmfile, \"r\")\n",
      "    street_types = defaultdict(set)\n",
      "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
      "\n",
      "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
      "            for tag in elem.iter(\"tag\"):\n",
      "                if is_street_name(tag):\n",
      "                    audit_street_type(street_types, tag.attrib['v'])\n",
      "\n",
      "    return street_types\n",
      "\n",
      "def update_name(name, mapping):\n",
      "    # YOUR CODE HERE\n",
      "    for element in mapping.keys():\n",
      "        if re.search(r'\\s*' + element + r'\\s*(?!\\S)', name):\n",
      "            name = name.replace(element, mapping[element])\n",
      "    return name\n",
      "\n",
      "# Execution\n",
      "OSMFILE = 'state-college_pennsylvania.osm'\n",
      "st_types = audit(OSMFILE)\n",
      "pprint.pprint(dict(st_types))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'522': set(['US 522']),\n",
        " '655': set(['State Route 655']),\n",
        " 'Ave': set(['Delaware Ave',\n",
        "             'E College Ave',\n",
        "             'W College Ave',\n",
        "             'W Freedom Ave']),\n",
        " 'Building': set(['Food Science Building',\n",
        "                  'Rider Building',\n",
        "                  'The 300 Building']),\n",
        " 'Center': set(['Northland Center']),\n",
        " 'Dr': set(['Premiere Dr']),\n",
        " 'Ln': set(['Sandy Ln']),\n",
        " 'Narrows': set(['Millheim Narrows']),\n",
        " 'Plaza': set(['Abby Plaza', 'Patton Plaza']),\n",
        " 'Rd': set([\"McAlevy's Fort Rd\"]),\n",
        " 'St': set(['4th St', 'E Main St', 'N Juniata St']),\n",
        " 'Terrace': set(['Johnson Terrace', 'Martin Terrace'])}\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2.1 Over-abbreviated Street Names"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some street names are over-abbreviated. For example, we should update Ave to Avenue, St to Street, Rd to Road and things like that. We should also update  single letter indicating direction to direction full name like: East, North.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2.2 Missing \"Street\" type"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some of the street type is missing. For example, 'US 522' should be 'US Route 522' and 'Millheim Narrows' should be 'Millheim Narrows Road'."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2.3 Wrong \"Street\" type"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some of the street type is totally wrong. For example, Some of users may include 'Building' or 'Terrace' as street type which should be fixed."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3. Data Overview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.1 File sizes\n",
      "state-college_pennsylvania.osm --------------- 88.1 MB  \n",
      "state-college_pennsylvania.osm.json --------- 99.0 MB  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.2 Total Tags"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load mapparser.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import xml.etree.ElementTree as ET\n",
      "import pprint\n",
      "\n",
      "def count_tags(filename):\n",
      "    # YOUR CODE HERE\n",
      "    tags={}\n",
      "    for event,element in ET.iterparse(filename):\n",
      "        if event=='end':\n",
      "            key = element.tag\n",
      "            if key not in tags:\n",
      "                tags[key] = 1\n",
      "            else:\n",
      "                tags[key] = tags[key] + 1\n",
      "    return tags\n",
      "\n",
      "# Execution\n",
      "tags = count_tags('state-college_pennsylvania.osm')\n",
      "pprint.pprint(tags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'bounds': 1,\n",
        " 'member': 6420,\n",
        " 'nd': 482392,\n",
        " 'node': 428081,\n",
        " 'osm': 1,\n",
        " 'relation': 508,\n",
        " 'tag': 208591,\n",
        " 'way': 32040}\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Data Transformation and Insertion\n",
      "Note: Since I have already transformed the data and inserted the data into MongoDB data base. Here I only show the code but I will not run the code on IPython Notebook.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load data.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import xml.etree.ElementTree as ET\n",
      "import pprint\n",
      "import re\n",
      "import codecs\n",
      "import json\n",
      "\n",
      "lower = re.compile(r'^([a-z]|_)*$')\n",
      "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
      "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
      "mapping = { \"St\": \"Street\",\n",
      "            \"St.\": \"Street\",\n",
      "            \"ln\": \"Lane\",\n",
      "            \"ln.\": \"Lane\",\n",
      "            \"Ln\": \"Lane\",\n",
      "            \"Ln.\": \"Lane\",\n",
      "            \"Ave\": \"Avenue\",\n",
      "            \"Avene\":\"Avenue\",\n",
      "            \"Rd.\":\"Road\",\n",
      "            \"Rd\":\"Road\",\n",
      "            \"RD\":\"Road\",\n",
      "            \"Dr\": \"Drive\",\n",
      "            \"Dr.\": \"Drive\",\n",
      "            \"Cir\":\"Circle\",\n",
      "            \"Blvd\":\"Boulevard\",\n",
      "            \"Blvd.\":\"Boulevard\",\n",
      "            \"Blvd,\":\"Boulevard\",\n",
      "            \"Blvd.,\":\"Boulevard\",\n",
      "            \"Cv\":\"Cove\",\n",
      "            \"Hwy\": \"Highway\",\n",
      "            \"Hwy.\": \"Highway\",\n",
      "            \"Ct\":\"Court\",\n",
      "            \"Ctr\":\"Court\",\n",
      "            \"CR\":\"Court\",\n",
      "            \"Ln\":\"Lane\",\n",
      "            \"Pkwy\":\"Parkway\",\n",
      "            \"Pky\": \"Parkway\",\n",
      "            \"Pky.\": \"Parkway\",\n",
      "            \"Fwy\": \"Freeway\",\n",
      "            \"Fwy.\": \"Freeway\",\n",
      "            \"E\": \"East\",\n",
      "            \"W\": \"West\",\n",
      "            \"N\": \"North\",\n",
      "            #\"S\": \"South\",\n",
      "            }\n",
      "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
      "\n",
      "def shape_element(element):\n",
      "    node = {}\n",
      "    created = {}\n",
      "    address = {}\n",
      "    node_refs = []\n",
      "    pos = [0, 0]\n",
      "    # you should process only 2 types of top level tags: \"node\" and \"way\"\n",
      "    if element.tag == \"node\" or element.tag == \"way\" :\n",
      "        if element.tag == \"node\":\n",
      "            node[\"type\"] = \"node\"\n",
      "        elif element.tag == \"way\":\n",
      "            node[\"type\"] = \"way\"\n",
      "        for key in element.attrib:\n",
      "            if key in CREATED:   # Created regular attributes\n",
      "                created[key] = element.attrib[key]\n",
      "            elif key == \"lat\":\n",
      "                pos[0] = float(element.attrib[\"lat\"])\n",
      "            elif key == \"lon\":\n",
      "                pos[1] = float(element.attrib[\"lon\"])\n",
      "            else:\n",
      "                node[key] = element.attrib[key]\n",
      "        node[\"created\"] = created   # attributes in the CREATED array should be added under a key \"created\"\n",
      "        node[\"pos\"] = pos          # attributes for latitude and longitude should be added to a \"pos\" array\n",
      "        for tag in element.iter(\"tag\"):\n",
      "            k_value = tag.attrib[\"k\"].strip()\n",
      "            v_value = tag.attrib[\"v\"].strip()\n",
      "            # if second level tag \"k\" value contains problematic characters, it should be ignored\n",
      "            if problemchars.search(k_value):\n",
      "                continue\n",
      "            # if second level tag \"k\" value starts with \"addr:\", it should be added to a dictionary \"address\"\n",
      "            elif \"addr:\" in k_value:\n",
      "                # if there is a second \":\" that separates the type/direction of a street, the tag should be ignored\n",
      "                if \":\" not in k_value[5:]:\n",
      "                    name = v_value\n",
      "                    name = name.replace(\",\", \"\")\n",
      "                    # if second level tag \"k\" value does not start with \"addr:\", but contains \":\", you can process it same as any other tag\n",
      "                    for word in name.split(\" \"):\n",
      "                        if word in mapping.keys():\n",
      "                            name = name.replace(word, mapping[word])\n",
      "                    address[k_value[5:].strip()] = name\n",
      "            elif k_value == \"shop\":\n",
      "                node[\"amenity\"] = v_value\n",
      "            elif k_value == \"sport\":\n",
      "                node[\"amenity\"] = v_value\n",
      "            elif k_value == \"tourism\":\n",
      "                node[\"amenity\"] = v_value\n",
      "            else:\n",
      "                 node[k_value] = tag.attrib[\"v\"].strip()\n",
      "        if address != {}:\n",
      "            node[\"address\"] = address\n",
      "        for nd in element.iter(\"nd\"):  # Add node_refs\n",
      "            node_refs.append(nd.attrib[\"ref\"])\n",
      "        if node_refs != []:\n",
      "            node[\"node_refs\"] = node_refs\n",
      "        return node\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "def process_map(file_in, pretty = False):\n",
      "    # You do not need to change this file\n",
      "    file_out = \"{0}.json\".format(file_in)\n",
      "    data = []\n",
      "    with codecs.open(file_out, \"w\") as fo:\n",
      "        for _, element in ET.iterparse(file_in):\n",
      "            el = shape_element(element)\n",
      "            if el:\n",
      "                data.append(el)\n",
      "                if pretty:\n",
      "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
      "                else:\n",
      "                    fo.write(json.dumps(el) + \"\\n\")\n",
      "    return data\n",
      "\n",
      "# Execution\n",
      "data = process_map('state-college_pennsylvania.osm', False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Insert Data\n",
      "from pymongo import MongoClient\n",
      "client  = MongoClient('mongodb://localhost:27017')\n",
      "db = client.udacity\n",
      "for item in data:\n",
      "\tdb.project2.insert(item)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we have transformed the data into JSON format and inserted into MongoDB data base. We can then do data aggregation using MongoDB"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.3 Number of Unique Users"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Data Aggregation\n",
      "import pprint\n",
      "from pymongo import MongoClient\n",
      "client  = MongoClient('mongodb://localhost:27017')\n",
      "db = client.udacity\n",
      "# number of unique users\n",
      "pprint.pprint(len(db.project2.distinct(\"created.user\")))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "293\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.4 Number of Nodes and Ways"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# number of nodes and ways\n",
      "pprint.pprint(db.project2.find({\"type\":\"node\"}).count())\n",
      "pprint.pprint(db.project2.find({\"type\":\"way\"}).count())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "428080\n",
        "32035"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.5 Number of Book Stores"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline1 = [\n",
      "\t\t\t{\"$match\": {\"amenity\": \"books\"}},\n",
      "\t\t\t{\"$project\":{\"_id\": \"$name\", \"opening_hours\": \"$opening_hours\"}}]\n",
      "pprint.pprint(len(db.project2.aggregate(pipeline1)['result']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.6 Number of Soccer Fields"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# number of soccer fields\n",
      "pipeline2 = [\n",
      "\t\t\t{\"$match\": {\"amenity\": \"soccer\"}},\n",
      "\t\t\t{\"$project\":{\"_id\": \"$name\", \"surface\": \"$surface\"}}]\n",
      "pprint.pprint(len(db.project2.aggregate(pipeline2)['result']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.7 Number of Hotels"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline3 = [\n",
      "\t\t\t{\"$match\": {\"amenity\": \"hotel\"}},\n",
      "\t\t\t{\"$project\":{\"_id\": \"$name\", \"stars\": \"$stars\", \"rooms\": \"$rooms\"}}]\n",
      "pprint.pprint(len(db.project2.aggregate(pipeline3)['result']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.8 Top 10 Contributed Users"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline4 = [\n",
      "            {\"$match\": {\"created.user\":{\"$exists\":1}}},\n",
      "            {\"$group\": {\"_id\":\"$created.user\",\"count\":{\"$sum\":1}}},\n",
      "            {\"$sort\": {\"count\":-1}},\n",
      "            {\"$limit\" : 10}]\n",
      "pprint.pprint(db.project2.aggregate(pipeline4)['result'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{u'_id': u'woodpeck_fixbot', u'count': 195088},\n",
        " {u'_id': u'pkoby', u'count': 94007},\n",
        " {u'_id': u'Sven L', u'count': 78041},\n",
        " {u'_id': u'Matt1993', u'count': 25799},\n",
        " {u'_id': u'Aaron Dennis', u'count': 9327},\n",
        " {u'_id': u'TIGERcnl', u'count': 7738},\n",
        " {u'_id': u'choess', u'count': 6287},\n",
        " {u'_id': u'bot-mode', u'count': 6148},\n",
        " {u'_id': u'DaveHansenTiger', u'count': 4597},\n",
        " {u'_id': u'rickmastfan67', u'count': 3097}]\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4. Additional Ideas"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we actually run pipeline and check our thoughts, we should have a overview of amenities in this area, so taht we can know where we can dig more information from this data set.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Data Aggregation - Advanced\n",
      "pprint.pprint(len(db.project2.distinct(\"amenity\")))\n",
      "# Top 10 appearing amenities\n",
      "pipeline5 = [\n",
      "            {\"$match\": {\"amenity\":{\"$exists\":1}}},\n",
      "            {\"$group\": {\"_id\":\"$amenity\",\"count\":{\"$sum\":1}}},\n",
      "            {\"$sort\": {\"count\":-1}},\n",
      "            {\"$limit\" : 10}]\n",
      "pprint.pprint(db.project2.aggregate(pipeline5)['result'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "147\n",
        "["
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'_id': u'parking', u'count': 365},\n",
        " {u'_id': u'school', u'count': 177},\n",
        " {u'_id': u'bicycle_parking', u'count': 170},\n",
        " {u'_id': u'place_of_worship', u'count': 124},\n",
        " {u'_id': u'restaurant', u'count': 117},\n",
        " {u'_id': u'grave_yard', u'count': 101},\n",
        " {u'_id': u'tennis', u'count': 75},\n",
        " {u'_id': u'fast_food', u'count': 53},\n",
        " {u'_id': u'golf', u'count': 46},\n",
        " {u'_id': u'clothes', u'count': 45}]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we see, we have 147 distinct amenities. We also have Top 10 appearing amenities, and they are what we want to find out more. However, things were not going well as I expected. In this dataset, there are a lot of information loss. Lots of information that was indicated in the Map Features (http://wiki.openstreetmap.org/wiki/Map_Features) were actually none. For example, most of the 'parking' and 'bicycle_parking' don't have information about capacity and fee. and most of 'restaurant' doesn't have information about 'stars'. Thus my data aggregation work was limited. Therefore, my work for additional ideas may have some similar parts as sample project. I think the sample project was limited on those topics because the same reason. I have tried my best to avoid that."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4.1 Top 5 Fast Food Brand"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Top 5 fast food brand\n",
      "pipeline6 = [\n",
      "\t\t\t{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\":\"fast_food\"}},\n",
      "\t\t\t{\"$group\": {\"_id\":\"$name\",\"count\":{\"$sum\":1}}},\n",
      "\t\t\t{\"$sort\": {\"count\":-1}},\n",
      "            {\"$limit\" : 5}]\n",
      "pprint.pprint(db.project2.aggregate(pipeline6)['result'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{u'_id': u\"McDonald's\", u'count': 8},\n",
        " {u'_id': u'Subway', u'count': 4},\n",
        " {u'_id': u'Dairy Queen', u'count': 3},\n",
        " {u'_id': u'Burger King', u'count': 3},\n",
        " {u'_id': u'Taco Bell', u'count': 3}]\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4.2 Top 5 Restaurant Cuisine"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Top 5 restaurant cuisine\n",
      "pipeline7 = [\n",
      "\t\t\t{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\":\"restaurant\"}},\n",
      "\t\t\t{\"$group\": {\"_id\":\"$cuisine\",\"count\":{\"$sum\":1}}},\n",
      "\t\t\t{\"$sort\": {\"count\":-1}},\n",
      "            {\"$limit\" : 10}]\n",
      "pprint.pprint(db.project2.aggregate(pipeline7)['result'][1:6])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{u'_id': u'pizza', u'count': 8},\n",
        " {u'_id': u'american', u'count': 7},\n",
        " {u'_id': u'mexican', u'count': 6},\n",
        " {u'_id': u'chinese', u'count': 5},\n",
        " {u'_id': u'italian', u'count': 5}]\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4.3 Biggest Religion"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Biggest religion\n",
      "pipeline8 = [\n",
      "\t\t\t{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\":\"place_of_worship\"}},\n",
      "\t\t\t{\"$group\": {\"_id\":\"$religion\",\"count\":{\"$sum\":1}}},\n",
      "\t\t\t{\"$sort\": {\"count\":-1}},\n",
      "            {\"$limit\" : 10}]\n",
      "pprint.pprint(db.project2.aggregate(pipeline8)['result'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{u'_id': u'christian', u'count': 122},\n",
        " {u'_id': u'muslim', u'count': 1},\n",
        " {u'_id': None, u'count': 1}]\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4.4 Top 10 Highest Elevation of Grave Yard"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Top 10 Highest Elevation of grave_yard\n",
      "pipeline9 = [\n",
      "\t\t\t{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\":\"grave_yard\"}},\n",
      "\t\t\t{\"$project\": {\"_id\":\"$name\",\"ele\":\"$ele\"}},\n",
      "\t\t\t{\"$sort\": {\"ele\":-1}},\n",
      "            {\"$limit\" : 10}]\n",
      "pprint.pprint(db.project2.aggregate(pipeline9)['result'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{u'_id': u'Butler Cemetery', u'ele': u'597'},\n",
        " {u'_id': u'Mountain Cemetery', u'ele': u'533'},\n",
        " {u'_id': u'Center Hill Cemetery', u'ele': u'521'},\n",
        " {u'_id': u'Kylertown Cemetery', u'ele': u'515'},\n",
        " {u'_id': u'New Cemetery', u'ele': u'509'},\n",
        " {u'_id': u'Hunter Cemetery', u'ele': u'509'},\n",
        " {u'_id': u'Askey Cemetery', u'ele': u'504'},\n",
        " {u'_id': u'Philipsburg Cemetery', u'ele': u'497'},\n",
        " {u'_id': u'Saint Agnes Cemetery', u'ele': u'494'},\n",
        " {u'_id': u'Emigh Cemetery', u'ele': u'494'}]\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5. Conclusion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Through this data wrangling work, I have found three main problems in this data set. and there are lots of information are incomplete.  \n",
      "Thank you for reading my project paper.  "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}